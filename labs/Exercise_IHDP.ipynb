{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise IHDP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbJjcK5ZEqvfC3aCmGI0Az",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmachlanski/iads-summer-school-causality-2023/blob/main/labs/Exercise_IHDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBoPcfD9l9V8"
      },
      "source": [
        "# Causal Inference - Exercise (IHDP)\n",
        "\n",
        "This is an opportunity for everyone to put into practice everything we have learnt so far. The data for this exercise comes from Infant Health Development Program study and was modified specifically for causal inference estimation purposes. More precisely, this dataset was formally introduced by [Hill (2011)](https://doi.org/10.1198/jcgs.2010.08162). It is a commonly used semi-simulated dataset in the CI community that combines pre-treatment covariates (X) and treatment assignments (T) from a real study, and simulated outcomes (Y). Because all outcomes are generated (both $y_1$ and $y_0$), we can measure individual as well as average treatment effect errors. For training purposes, only one of the outcomes is available to the estimator. The other is hidden and used only for evaluation purposes.\n",
        "\n",
        "The experiment where the covariates come from measured various aspects of premature infants and their mothers, and how receiving specialised childcare affected the cognitive test score of the infants later on. The treatment groups are made imbalanced by removing a subset of the treated individuals. The variables are a mixture of contonuous and binary features. Treatment is binary. The outcome Y is continuous. Overall, we have 25 background features X. The data consists of 747 samples (139 treated, 608 control)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcflqEHyquI_"
      },
      "source": [
        "In terms of evaluation metrics, we are interested in predicting both individual and average treatment effects for this task. As the outcomes are simulated, we have access to both true outcomes $\\mathcal{Y}_1$ and $\\mathcal{Y}_0$ for each individual (i). As a result, we have access to true ITEs and true ATE:\n",
        "\n",
        "$$ITE^{(i)} = \\mathcal{Y}_1^{(i)} - \\mathcal{Y}_0^{(i)}$$\n",
        "\n",
        "$$ATE = \\mathbb{E}[ITE]$$\n",
        "\n",
        "We can define our predictions as:\n",
        "\n",
        "$$\\widehat{ITE}^{(i)} = \\hat{y}_1^{(i)} - \\hat{y}_0^{(i)}$$\n",
        "\n",
        "$$\\widehat{ATE} = \\frac{1}{n}\\sum \\limits_{i=1}^{n}\\widehat{ITE}^{(i)}$$\n",
        "\n",
        "This allows us to define measurement errors with respect to each as:\n",
        "\n",
        "$$\\epsilon_{PEHE} = \\sqrt{\\frac{1}{n}\\sum \\limits_{i=1}^{n}(\\widehat{ITE}^{(i)} - ITE^{(i)})^2}$$\n",
        "\n",
        "$$\\epsilon_{ATE} = \\left| \\widehat{ATE} - ATE \\right|$$\n",
        "\n",
        "Where PEHE stands for Precision in Estimation of Heterogeneous Effect, and which essentially is a Root Mean Squared Error (RMSE) between predicted and true ITEs. Implementations of both metrics are provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao4shEd7IRNb"
      },
      "source": [
        "def rmse(a, b):\n",
        "    return np.sqrt(((a - b)**2).mean())\n",
        "\n",
        "def ate_error(pred_te, true_te):\n",
        "  return np.abs(np.mean(pred_te) - np.mean(true_te))\n",
        "\n",
        "def pehe_error(pred_te, true_te):\n",
        "  return rmse(true_te, pred_te)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-3Z9QlR1tdA"
      },
      "source": [
        "We suggest the following steps:\n",
        "1. Import packages.\n",
        "    1. Remember to install EconML if you want to use it.\n",
        "2. Data.\n",
        "    1. Can be accessed at the following URLs:\n",
        "        1. Training: https://github.com/dmachlanski/iads-summer-school-causality-2023/raw/main/labs/data/ihdp_train.npz\n",
        "        2. Testing: https://github.com/dmachlanski/iads-summer-school-causality-2023/raw/main/labs/data/ihdp_test.npz\n",
        "    2. Use 'wget' command to download them into the notebook (or upload manually).\n",
        "    3. Explore the data (print a few samples, plot distributions - see plot_dist function below).\n",
        "3. Data pre-processing.\n",
        "    1. No data splitting required (train and test already provided).\n",
        "    2. Scaling.\n",
        "4. Train estimators of your choice (re-use already presented ones or explore different methods.\n",
        "  1. EconML - [CATE estimators](https://econml.azurewebsites.net/reference.html#cate-estimators).\n",
        "  2. scikit-learn - [supervised methods](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).\n",
        "5. Make predictions.\n",
        "6. Evaluate your models.\n",
        "    1. Measure ATE and PEHE errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcy1W32ix3o8"
      },
      "source": [
        "def _trim_axs(axs, N):\n",
        "    axs = axs.flat\n",
        "    for ax in axs[N:]:\n",
        "        ax.remove()\n",
        "    return axs[:N]\n",
        "\n",
        "def plot_dist(data, bins=10):\n",
        "    \"\"\"\n",
        "    data: 2-dimensional numpy array\n",
        "    bins: number of bins in the histograms\n",
        "    \"\"\"\n",
        "    if data.shape[1] > 1:\n",
        "        sq = math.sqrt(data.shape[1])\n",
        "        d_ceil = math.ceil(sq)\n",
        "        d_floor = math.floor(sq)\n",
        "\n",
        "        if (d_ceil * d_floor) >= data.shape[1]:\n",
        "            n_rows = d_floor\n",
        "            n_cols = d_ceil\n",
        "        else:\n",
        "            n_rows = n_cols = d_ceil\n",
        "\n",
        "        _, axs = plt.subplots(n_rows, n_cols)\n",
        "        axs = _trim_axs(axs, data.shape[1])\n",
        "\n",
        "        for i, ax in enumerate(axs):\n",
        "            ax.hist(data[:, i], bins=bins)\n",
        "    else:\n",
        "        plt.hist(data, bins=bins)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}